<?xml version="1.0" encoding="UTF-8"?>
<Tokens version="1.0">
	<File path="Classes/AWSRekognitionGetFaceSearchResponse.html">
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/cl/AWSRekognitionGetFaceSearchResponse</TokenIdentifier>
			<Abstract type="html"></Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
            
			
			<NodeRef refid="1282"/>
		</Token>
		
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/setJobStatus:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;The current status of the face search job.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionVideoJobStatus jobStatus</Declaration>
			
			
			<Anchor>//api/name/jobStatus</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/jobStatus</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;The current status of the face search job.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionVideoJobStatus jobStatus</Declaration>
			
			
			<Anchor>//api/name/jobStatus</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionGetFaceSearchResponse/jobStatus</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;The current status of the face search job.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, assign) AWSRekognitionVideoJobStatus jobStatus</Declaration>
			
			
			<Anchor>//api/name/jobStatus</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/setNextToken:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;If the response is truncated, Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSString *nextToken</Declaration>
			
			
			<Anchor>//api/name/nextToken</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/nextToken</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;If the response is truncated, Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSString *nextToken</Declaration>
			
			
			<Anchor>//api/name/nextToken</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionGetFaceSearchResponse/nextToken</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;If the response is truncated, Rekognition Video returns this token that you can use in the subsequent request to retrieve the next set of search results. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSString *nextToken</Declaration>
			
			
			<Anchor>//api/name/nextToken</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/setPersons:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of persons, , in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information for when persons are matched in the video. You specify the input collection in an initial call to &lt;code&gt;StartFaceSearch&lt;/code&gt;. Each &lt;code&gt;Persons&lt;/code&gt; element includes a time the person was matched, face match details (&lt;code&gt;FaceMatches&lt;/code&gt;) for matching faces in the collection, and person information (&lt;code&gt;Person&lt;/code&gt;) for the matched person. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionPersonMatch*&gt; *persons</Declaration>
			
			
			<Anchor>//api/name/persons</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/persons</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of persons, , in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information for when persons are matched in the video. You specify the input collection in an initial call to &lt;code&gt;StartFaceSearch&lt;/code&gt;. Each &lt;code&gt;Persons&lt;/code&gt; element includes a time the person was matched, face match details (&lt;code&gt;FaceMatches&lt;/code&gt;) for matching faces in the collection, and person information (&lt;code&gt;Person&lt;/code&gt;) for the matched person. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionPersonMatch*&gt; *persons</Declaration>
			
			
			<Anchor>//api/name/persons</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionGetFaceSearchResponse/persons</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;An array of persons, , in the video whose face(s) match the face(s) in an Amazon Rekognition collection. It also includes time information for when persons are matched in the video. You specify the input collection in an initial call to &lt;code&gt;StartFaceSearch&lt;/code&gt;. Each &lt;code&gt;Persons&lt;/code&gt; element includes a time the person was matched, face match details (&lt;code&gt;FaceMatches&lt;/code&gt;) for matching faces in the collection, and person information (&lt;code&gt;Person&lt;/code&gt;) for the matched person. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSArray&lt;AWSRekognitionPersonMatch*&gt; *persons</Declaration>
			
			
			<Anchor>//api/name/persons</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/setStatusMessage:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;If the job fails, &lt;code&gt;StatusMessage&lt;/code&gt; provides a descriptive error message.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSString *statusMessage</Declaration>
			
			
			<Anchor>//api/name/statusMessage</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/statusMessage</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;If the job fails, &lt;code&gt;StatusMessage&lt;/code&gt; provides a descriptive error message.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSString *statusMessage</Declaration>
			
			
			<Anchor>//api/name/statusMessage</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionGetFaceSearchResponse/statusMessage</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;If the job fails, &lt;code&gt;StatusMessage&lt;/code&gt; provides a descriptive error message.&lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) NSString *statusMessage</Declaration>
			
			
			<Anchor>//api/name/statusMessage</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/setVideoMetadata:</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;Information about a video that Amazon Rekognition analyzed. &lt;code&gt;Videometadata&lt;/code&gt; is returned in every page of paginated responses from a Rekognition Video operation. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) AWSRekognitionVideoMetadata *videoMetadata</Declaration>
			
			
			<Anchor>//api/name/videoMetadata</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instm/AWSRekognitionGetFaceSearchResponse/videoMetadata</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;Information about a video that Amazon Rekognition analyzed. &lt;code&gt;Videometadata&lt;/code&gt; is returned in every page of paginated responses from a Rekognition Video operation. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) AWSRekognitionVideoMetadata *videoMetadata</Declaration>
			
			
			<Anchor>//api/name/videoMetadata</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
		<Token>
			<TokenIdentifier>//apple_ref/occ/instp/AWSRekognitionGetFaceSearchResponse/videoMetadata</TokenIdentifier>
			<Abstract type="html">&lt;p&gt;Information about a video that Amazon Rekognition analyzed. &lt;code&gt;Videometadata&lt;/code&gt; is returned in every page of paginated responses from a Rekognition Video operation. &lt;/p&gt;</Abstract>
			<DeclaredIn>AWSRekognitionModel.h</DeclaredIn>
			
			<Declaration>@property (nonatomic, strong) AWSRekognitionVideoMetadata *videoMetadata</Declaration>
			
			
			<Anchor>//api/name/videoMetadata</Anchor>
            <NodeRef refid="1282"/>
		</Token>
		
        
        
	</File>
</Tokens>